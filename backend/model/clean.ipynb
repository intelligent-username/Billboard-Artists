{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c9e4db5",
   "metadata": {},
   "source": [
    "# Part 1: Data Cleaning\n",
    "\n",
    "For the prediction model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0975b94",
   "metadata": {},
   "source": [
    "## Step 1: Isolate Debuts\n",
    "Database: billboard_global_200.csv<br>\n",
    "All of the Global 200 rankings on Billboard since it's beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267fed0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4325 unique songs written.\n",
      "Data cleaning phase 1 done.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "address = \"../data/billboard_global_200.csv\"\n",
    "first_appearance_addy = \"../data/clean1.csv\"\n",
    "\n",
    "seen = set()\n",
    "\n",
    "def normalize(s: str) -> str:\n",
    "    return re.sub(r'\\s+', ' ', s.lower().strip())\n",
    "\n",
    "def canonical_key(row):\n",
    "    # row layout: [date, rank, title, main_artist, featured_artists]\n",
    "    song = normalize(row[2])\n",
    "\n",
    "    artist_blob = f\"{row[3]},{row[4]}\"\n",
    "    artists = re.split(\n",
    "        r',|&|\\+|and|feat\\.?|ft\\.?|featuring|x|with|w/',\n",
    "        artist_blob,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    artists = sorted({\n",
    "        normalize(a) for a in artists\n",
    "        if a and a.strip() and a.strip().lower() not in {\"n/a\", \"none\"}\n",
    "    })\n",
    "\n",
    "    return (song, tuple(artists))\n",
    "\n",
    "\n",
    "with open(address, 'r', encoding='utf-8') as file, \\\n",
    "     open(first_appearance_addy, 'w', newline='', encoding='utf-8') as write_file:\n",
    "\n",
    "    reader = csv.reader(file)\n",
    "    writer = csv.writer(write_file)\n",
    "\n",
    "    header = next(reader)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for row in reader:\n",
    "        if len(row) < 5:\n",
    "            continue\n",
    "\n",
    "        key = canonical_key(row)\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            writer.writerow(row)\n",
    "\n",
    "print(f\"{len(seen)} unique songs written.\")\n",
    "print(\"Data cleaning phase 1 done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df31b470",
   "metadata": {},
   "source": [
    "## Step 2: Sweep the Data\n",
    "- Convert dates to numerical format (decades since September 5th, 2020, caps the value at 1 and still be accurate)\n",
    "- Normalize ranking (keep b/w 0 and 1), w/ #1 rank being 1 and #200 rank being 0, so greater is better.\n",
    "- Separate collaborations properly(!!!)\n",
    "- Replace N/A values (when applicable) with \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55c859f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning phase 2\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import math\n",
    "import re\n",
    "\n",
    "address = \"../data/clean1.csv\"\n",
    "target = \"../data/clean2.csv\"\n",
    "\n",
    "columns = \"decades_since_2020_Sept,date,rank, rank_norm, title,main_artist,featured_artists\"\n",
    "\n",
    "day_one = datetime.datetime(2020, 9, 5)\n",
    "\n",
    "# Helpers\n",
    "def graft_exceptions(artist_list):\n",
    "    def is_exception(candidate):\n",
    "        # Exact-match exceptions\n",
    "        if candidate in EXCEPTIONS:\n",
    "            return True\n",
    "        # Ends with \"And His Orchestra\"\n",
    "        if candidate.endswith(\"And His Orchestra\"):\n",
    "            return True\n",
    "        # Match &-based exceptions ignoring spaces around &\n",
    "        candidate_norm = candidate.replace(' & ', '&')\n",
    "        for ex in EXCEPTIONS:\n",
    "            if ex.replace(' & ', '&') == candidate_norm:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    if len(artist_list) <= 1:\n",
    "        return artist_list\n",
    "    \n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(artist_list):\n",
    "        max_look = min(i + 4, len(artist_list))\n",
    "        found = False\n",
    "        for j in range(max_look, i, -1):\n",
    "            candidate = ' '.join(artist_list[i:j])  # <--- change here\n",
    "            if is_exception(candidate):\n",
    "                result.append(candidate)\n",
    "                i = j\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            result.append(artist_list[i])\n",
    "            i += 1\n",
    "    return result\n",
    "\n",
    "def split_artists(main, sec):\n",
    "    def normalize_artists(s, sec_column_nonempty=False):\n",
    "        if not s or s.strip() in {'N/A', 'None'}:\n",
    "            return ''\n",
    "        if sec_column_nonempty:\n",
    "            s = re.sub(r'(feat\\.?|ft\\.?|featuring|Featuring)', ' feat. ', s)\n",
    "        s = re.sub(r'\\s*x\\s*', ' x ', s)\n",
    "        s = re.sub(r'(?i)(\\S)With(\\S)', r'\\1 With \\2', s)\n",
    "        s = re.sub(r'(?i)\\s*With\\s*', ' With ', s)\n",
    "        s = re.sub(r'\\s+', ' ', s)\n",
    "        return s.strip()\n",
    "\n",
    "\n",
    "    def split_names(names, is_feature=False):\n",
    "        names = normalize_artists(names)\n",
    "        if not names:\n",
    "            return []\n",
    "\n",
    "        # If the whole string matches an exception, skip splitting\n",
    "        if is_feature:\n",
    "            if names in EXCEPTIONS or names.endswith(\"And His Orchestra\"):\n",
    "                return [names]\n",
    "\n",
    "        # otherwise, split normally\n",
    "        result = [names]\n",
    "        for splitter in SPLITTERS:\n",
    "            temp = []\n",
    "            for name in result:\n",
    "                temp.extend([n.strip() for n in name.split(splitter)])\n",
    "            result = temp\n",
    "\n",
    "        seen = set()\n",
    "        unique = []\n",
    "        for n in result:\n",
    "            if n and n not in seen:\n",
    "                seen.add(n)\n",
    "                unique.append(n)\n",
    "        return graft_exceptions(unique)\n",
    "\n",
    "\n",
    "\n",
    "    main_artists = split_names(normalize_artists(main, bool(sec.strip() and sec.strip() != \"N/A\")))\n",
    "    sec_artists = split_names(normalize_artists(sec), is_feature=True)\n",
    "\n",
    "    if len(main_artists) > 1:\n",
    "        sec_artists = main_artists[1:] + sec_artists\n",
    "        main_artists = [main_artists[0]]\n",
    "\n",
    "    return main_artists, sec_artists\n",
    "\n",
    "\n",
    "with open(address, 'r', encoding='utf-8') as file, \\\n",
    "     open(target, 'w', newline='', encoding='utf-8') as write_file:\n",
    "\n",
    "    cursor = csv.reader(file)\n",
    "    writer = csv.writer(write_file)\n",
    "    writer.writerow(columns.split(','))\n",
    "    next(cursor)\n",
    "\n",
    "    SPLITTERS = [',', '&', ' + ', ' and ',\n",
    "                 ' feat. ', ' ft. ', ' featuring ', ' Featuring ',\n",
    "                 ' x ', ' X ', ' with ', ' With ', 'Duet With', 'w/', \"/\"]\n",
    "\n",
    "    EXCEPTIONS = {\n",
    "        \"Tyler, the Creator\",\n",
    "        \"Tyler, The Creator\",\n",
    "\n",
    "        \"John Scott Trotter & His Orchestra\",\n",
    "        \"Ralph Carmichael Orchestra and Chorus\",\n",
    "        \"Georgie Stoll & His Orchestra\",\n",
    "\n",
    "        \"AC/DC\",\n",
    "        \"Earth, Wind & Fire\",\n",
    "        \"TWS: 24/7\",\n",
    "        \"HUNTR/X\",\n",
    "    }\n",
    "\n",
    "    for row in cursor:\n",
    "        try:\n",
    "            date = datetime.datetime.strptime(row[0], \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        diff = ((date - day_one).days / 3652) + 0.000001\n",
    "\n",
    "        try:\n",
    "            rank = int(row[1])\n",
    "            rank_norm = 1 / math.sqrt(rank)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        main = row[3]\n",
    "        sec = row[4]\n",
    "\n",
    "        n1, n2 = split_artists(main, sec)\n",
    "\n",
    "        main_out = f\"'{n1[0]}'\" if n1 else \"'None'\"\n",
    "        sec_out = '\"' + \", \".join([f\"'{artist}'\" for artist in n2]) + '\"' if n2 else '\"None\"'\n",
    "\n",
    "        writer.writerow([diff, row[0], row[1], rank_norm, row[2], main_out, sec_out])\n",
    "\n",
    "print(\"Finished cleaning phase 2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1ad52f",
   "metadata": {},
   "source": [
    "## Step 3: Engineer Some Features\n",
    "\n",
    "- Use embeddings for artist names to see which artists are closer to each other\n",
    "- Create charting recency bias (more recent data weighted more heavily), using numerical format of date\n",
    "\n",
    "**Feature Engineering**\n",
    "- For each song, aggregate the past performance of the main artist up to that point\n",
    "- For each song, aggregate the combined past performance of collaborators (again, w/ recency bias)\n",
    "\n",
    "Keep in mind release date and song title will be ignored but kept in case in the future I want to use this data.\n",
    "\n",
    "* In the future, possibly use non-debut data to examine 'longevity' of artists (might be a waste of time). That is, don't ONLY consider the debut of a given artist's songs, just consider it more heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab99460f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Phase 3\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "import re\n",
    "\n",
    "address = \"../data/clean2.csv\"\n",
    "target = \"../data/clean3.csv\"\n",
    "\n",
    "new_cols = \"decades_since_2020_Sept,date,rank,rank_norm,title,main_artist,featured_artists,artist_past_performance,features_past_performance\"\n",
    "\n",
    "DECAY_LAMBDA = 0.1\n",
    "FEATURES_DEFAULT = 0.5\n",
    "\n",
    "artist_pat = re.compile(r\"'([^']*)'\")\n",
    "COMMA_SPLIT = re.compile(r'\\s*,\\s*')\n",
    "\n",
    "\n",
    "def parse_time(s: str):\n",
    "    if not s:\n",
    "        return None\n",
    "    s = s.strip()\n",
    "    try:\n",
    "        return float(s)\n",
    "    except ValueError:\n",
    "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "            try:\n",
    "                return float(s[1:-1].strip())\n",
    "            except:\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_main(name: str) -> str:\n",
    "    if not name:\n",
    "        return \"\"\n",
    "    return name.strip().strip(\"'\").strip()\n",
    "\n",
    "\n",
    "def parse_features(s: str) -> list[str]:\n",
    "    if not s or s in ('None', '\"None\"'):\n",
    "        return []\n",
    "    found = artist_pat.findall(s)\n",
    "    if found:\n",
    "        return [f.strip() for f in found if f.strip()]\n",
    "    inner = s[1:-1].strip() if s.startswith('\"') and s.endswith('\"') else s\n",
    "    return [p.strip().strip(\"'\").strip().strip('\"') for p in COMMA_SPLIT.split(inner) if p.strip()]\n",
    "\n",
    "\n",
    "def decay_to(t_now: float, state_tuple):\n",
    "    S, W, t0 = state_tuple\n",
    "    if t_now is None:\n",
    "        return S, W, t0\n",
    "    if t_now <= t0:\n",
    "        return S, W, t0\n",
    "    decay = math.exp(-DECAY_LAMBDA * (t_now - t0))\n",
    "    return S * decay, W * decay, t_now\n",
    "\n",
    "\n",
    "with open(address, \"r\", newline=\"\", encoding=\"utf-8\") as f_in, \\\n",
    "     open(target, \"w\", newline=\"\", encoding=\"utf-8\") as f_out:\n",
    "\n",
    "    reader = csv.reader(f_in)\n",
    "    writer = csv.writer(f_out)\n",
    "    writer.writerow([c.strip() for c in new_cols.split(\",\")])\n",
    "    next(reader, None)  # skip header\n",
    "\n",
    "    rows = list(reader)\n",
    "\n",
    "    # stable chronological sort\n",
    "    def sort_key(row):\n",
    "        t = parse_time(row[0])\n",
    "        t_sort = t if t is not None else float(\"inf\")\n",
    "        date = row[1] if len(row) > 1 else \"\"\n",
    "        try:\n",
    "            rank_int = int(row[2]) if len(row) > 2 and row[2].strip() != \"\" else 9999\n",
    "        except:\n",
    "            rank_int = 9999\n",
    "        return (t_sort, date, rank_int)\n",
    "\n",
    "    rows.sort(key=sort_key)\n",
    "\n",
    "    # per-artist state: artist -> (S, W, last_time)\n",
    "    state = {}\n",
    "\n",
    "    for row in rows:\n",
    "        if len(row) < 7:\n",
    "            continue\n",
    "\n",
    "        t = parse_time(row[0])\n",
    "        if t is None:\n",
    "            continue\n",
    "\n",
    "        # normalized rank fallback\n",
    "        try:\n",
    "            r_norm = float(row[3])\n",
    "        except:\n",
    "            try:\n",
    "                rank_raw = int(row[2])\n",
    "                r_norm = 1.0 / math.sqrt(rank_raw) if rank_raw > 0 else 0.5\n",
    "            except:\n",
    "                r_norm = 0.5\n",
    "\n",
    "        main = parse_main(row[5])\n",
    "        feats = parse_features(row[6])\n",
    "\n",
    "        # **compute past performance using slower decay**\n",
    "        S_m, W_m, _ = decay_to(t, state.get(main, (0.0, 0.0, t)))\n",
    "        main_perf = (S_m / W_m) if W_m > 1e-8 else 0.5\n",
    "\n",
    "        feat_perfs = []\n",
    "        for a in feats:\n",
    "            S_a, W_a, _ = decay_to(t, state.get(a, (0.0, 0.0, t)))\n",
    "            feat_perfs.append((S_a / W_a) if W_a > 1e-8 else 0.5)\n",
    "        feats_perf = (sum(feat_perfs) / len(feat_perfs)) if feat_perfs else FEATURES_DEFAULT\n",
    "\n",
    "        writer.writerow(row + [main_perf, feats_perf])\n",
    "\n",
    "        # update states **incrementally without collapsing too aggressively**\n",
    "        for a in {main, *feats}:\n",
    "            S0, W0, t0 = state.get(a, (0.0, 0.0, t))\n",
    "            S0, W0, _ = decay_to(t, (S0, W0, t0))\n",
    "            S0 += r_norm\n",
    "            W0 += 1.0\n",
    "            last_time = max(t, t0)\n",
    "            state[a] = (S0, W0, last_time)\n",
    "\n",
    "print(\"Finished Phase 3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c8826b",
   "metadata": {},
   "source": [
    "## Step 4: Centrality & Rank\n",
    "\n",
    "\"Nicer\" features that actually capture the connectednes the way I want it to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac42cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This takes a long time to run (~2 minutes), will vectorize, clean up, and freshen later.\n",
    "\n",
    "import csv\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "INPUT = \"../data/clean3.csv\"\n",
    "OUTPUT = \"../data/clean4.csv\"\n",
    "\n",
    "DECAY_LAMBDA = 0.15          # temporal decay for edges\n",
    "PR_ALPHA = 0.85              # PageRank damping\n",
    "MIN_EDGE_WEIGHT = 1e-6\n",
    "\n",
    "artist_pat = re.compile(r\"'([^']*)'\")\n",
    "COMMA_SPLIT = re.compile(r'\\s*,\\s*')\n",
    "\n",
    "def parse_time(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def parse_main(x: str) -> str:\n",
    "    return x.strip().strip(\"'\") if x else \"\"\n",
    "\n",
    "def parse_features(x: str) -> List[str]:\n",
    "    if not x or x in (\"None\", '\"None\"'):\n",
    "        return []\n",
    "    found = artist_pat.findall(x)\n",
    "    if found:\n",
    "        return [f.strip() for f in found]\n",
    "    return [\n",
    "        p.strip().strip(\"'\").strip('\"')\n",
    "        for p in COMMA_SPLIT.split(x)\n",
    "        if p.strip()\n",
    "    ]\n",
    "\n",
    "# (a, b) -> (weight, last_time)\n",
    "EdgeState = Dict[Tuple[str, str], Tuple[float, float]]\n",
    "\n",
    "def decay(w, dt):\n",
    "    return w * math.exp(-DECAY_LAMBDA * dt)\n",
    "\n",
    "\n",
    "with open(INPUT, newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)\n",
    "    rows = list(reader)\n",
    "\n",
    "def sort_key(row):\n",
    "    t = parse_time(row[0])\n",
    "    return t if t is not None else float(\"inf\")\n",
    "\n",
    "rows.sort(key=sort_key)\n",
    "\n",
    "#State\n",
    "edges: EdgeState = {}\n",
    "last_global_time = None\n",
    "\n",
    "out_rows = []\n",
    "\n",
    "for row in rows:\n",
    "    t = parse_time(row[0])\n",
    "    if t is None:\n",
    "        continue\n",
    "\n",
    "    main = parse_main(row[5])\n",
    "    feats = parse_features(row[6])\n",
    "    artists = [main] + feats\n",
    "    artists = [a for a in artists if a]\n",
    "\n",
    "    # --- decay all edges forward in time ---\n",
    "    if last_global_time is not None:\n",
    "        dt = t - last_global_time\n",
    "        if dt > 0:\n",
    "            for k, (w, t0) in list(edges.items()):\n",
    "                w2 = decay(w, dt)\n",
    "                if w2 < MIN_EDGE_WEIGHT:\n",
    "                    del edges[k]\n",
    "                else:\n",
    "                    edges[k] = (w2, t)\n",
    "\n",
    "    last_global_time = t\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for (a, b), (w, _) in edges.items():\n",
    "        G.add_edge(a, b, weight=w)\n",
    "\n",
    "    if len(G) > 0:\n",
    "        deg = dict(G.degree(weight=\"weight\"))\n",
    "        pr = nx.pagerank(G, alpha=PR_ALPHA, weight=\"weight\")\n",
    "    else:\n",
    "        deg = {}\n",
    "        pr = {}\n",
    "\n",
    "    deg_main = deg.get(main, 0.0)\n",
    "    pr_main = pr.get(main, 0.0)\n",
    "\n",
    "    out_rows.append(row + [deg_main, pr_main])\n",
    "\n",
    "    for i in range(len(artists)):\n",
    "        for j in range(i + 1, len(artists)):\n",
    "            a, b = sorted((artists[i], artists[j]))\n",
    "            w0, _ = edges.get((a, b), (0.0, t))\n",
    "            edges[(a, b)] = (w0 + 1.0, t)\n",
    "\n",
    "\n",
    "with open(OUTPUT, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header + [\"deg_centrality\", \"pagerank\"])\n",
    "    writer.writerows(out_rows)\n",
    "\n",
    "print(\"Graph feature build complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
